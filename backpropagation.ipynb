{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tests_backpropagation import main_test\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class ``MyNet``\n",
    "\n",
    "Read carefully how ``MyNet`` is implemented in the cell below. In particular:  \n",
    "- ``n_hid`` is a list of integer, representing the number of hidden units in each hidden layer.   \n",
    "-  ``MyNet([2, 3, 2]) = MiniNet()`` where ``MiniNet`` is the neural network defined in the fourth tutorial, in which notations are also clarified.     \n",
    "- ``model.L`` is the number of hidden layers, ``L``   \n",
    "- ``model.f[l]`` is the activation function of layer ``l``, $f^{[l]}$ (here ``torch.tanh``)   \n",
    "- ``model.df[l]`` is the derivative of the activation function, $f'^{[l]}$   \n",
    "- ``model.a[l]``  is the tensor $A^{[l]}$, (shape: ``(1, n(l))``)   \n",
    "- ``model.z[l]``  is the tensor $Z^{[l]}$, (shape: ``(1, n(l))``)  \n",
    "- Weights $W^{[l]}$ (shape: ``(n(l+1), n(l))``) and biases $\\mathbf{b}^{[l]}$ (shape: ``(n(l+1))``) can be accessed as follows:\n",
    "```\n",
    "weights = model.fc[str(l)].weight.data\n",
    "bias = model.fc[str(l)].bias.data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, n_l = [2, 3, 2]):\n",
    "        super().__init__() \n",
    "        \n",
    "        \n",
    "        # number of layers in our network (following Andrew's notations)\n",
    "        self.L = len(n_l)-1\n",
    "        self.n_l = n_l\n",
    "        \n",
    "        # Where we will store our neuron values\n",
    "        # - z: before activation function \n",
    "        # - a: after activation function (a=f(z))\n",
    "        self.z = {i : None for i in range(1, self.L+1)}\n",
    "        self.a = {i : None for i in range(self.L+1)}\n",
    "\n",
    "        # Where we will store the gradients for our custom backpropagation algo\n",
    "        self.dL_dw = {i : None for i in range(1, self.L+1)}\n",
    "        self.dL_db = {i : None for i in range(1, self.L+1)}\n",
    "\n",
    "        # Our activation functions\n",
    "        self.f = {i : lambda x : torch.tanh(x) for i in range(1, self.L+1)}\n",
    "\n",
    "        # Derivatives of our activation functions\n",
    "        self.df = {\n",
    "            i : lambda x : (1 / (torch.cosh(x)**2)) \n",
    "            for i in range(1, self.L+1)\n",
    "        }\n",
    "        \n",
    "        # fully connected layers\n",
    "        # We have to use nn.ModuleDict and to use strings as keys here to \n",
    "        # respect pytorch requirements (otherwise, the model does not learn)\n",
    "        self.fc = nn.ModuleDict({str(i): None for i in range(1, self.L+1)})\n",
    "        for i in range(1, self.L+1):\n",
    "            self.fc[str(i)] = nn.Linear(in_features=n_l[i-1], out_features=n_l[i])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input layer\n",
    "        self.a[0] = torch.flatten(x, 1)\n",
    "        \n",
    "        # Hidden layers until output layer\n",
    "        for i in range(1, self.L+1):\n",
    "\n",
    "            # fully connected layer\n",
    "            self.z[i] = self.fc[str(i)](self.a[i-1])\n",
    "            # activation\n",
    "            self.a[i] = self.f[i](self.z[i])\n",
    "\n",
    "        # return output\n",
    "        return self.a[self.L]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Write a function ``backpropagation(model, y_true, y_pred)`` that computes:\n",
    "\n",
    "- $\\frac{\\partial L}{\\partial w^{[l]}_{i,j}}$ and store them in ``model.dL_dw[l][i,j]`` for $l \\in [1 .. L]$ \n",
    "- $\\frac{\\partial L}{\\partial b^{[l]}_{j}}$ and store them in ``model.dL_db[l][j]`` for $l \\in [1 .. L]$ \n",
    "\n",
    "assuming ``model`` is an instance of the ``MyNet`` class.\n",
    "\n",
    "A vectorized implementation would be appreciated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(model, y_true, y_pred):\n",
    "\n",
    "    L = model.L  \n",
    "\n",
    "    with torch.no_grad():\n",
    "                \n",
    "        dL_da = 2 * (y_pred - y_true)  \n",
    "\n",
    "        delta = {}\n",
    "        delta[L] = dL_da * model.df[L](model.z[L])\n",
    "\n",
    "        for l in range(L-1, 0, -1):\n",
    "            delta[l] = (delta[l+1] @ model.fc[str(l+1)].weight) * model.df[l](model.z[l])\n",
    "\n",
    "        for l in range(1, L+1):\n",
    "            model.dL_dw[l] = delta[l].t() @ model.a[l-1]\n",
    "            model.dL_db[l] = delta[l].squeeze(0)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the cells below, and check the output\n",
    "\n",
    "- In the 1st cell, we use a toy dataset and the same architecture as the MiniNet class of the fourth tutorial. \n",
    "- In the 2nd cell, we use a few samples of the MNIST dataset with a consistent model architecture (``24x24`` black and white cropped images as input and ``10`` output classes). \n",
    "\n",
    "You can set ``verbose`` to ``True`` if you want more details about your computations versus what is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __________________________________________________________________ \n",
      "                          Check gradients                             \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: Gradients consistent with autograd's computations.\n",
      "\n",
      " TEST PASSED: Gradients consistent with finite differences computations.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                 Check that weights have been updated               \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: Weights have been updated.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                      Check computational graph                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: All parameters seem correctly attached to the computational graph!\n",
      "\n",
      " __________________________________________________________________ \n",
      "                             Conclusion                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " 4 / 4: ALL TEST PASSED :)\n"
     ]
    }
   ],
   "source": [
    "model = MyNet([2, 3, 2])\n",
    "\n",
    "main_test(backpropagation, model, verbose=False, data='toy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /Users/johnorbaek/Documents/UiB/8. semester/Deep Learning/Project 1 /Project 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error downloading train-images-idx3-ubyte.gz:\nTried https://ossci-datasets.s3.amazonaws.com/mnist/, got:\n<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1081)>\nTried http://yann.lecun.com/exdb/mnist/, got:\nHTTP Error 404: Not Found\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m \n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mCWD:\u001b[39m\u001b[33m'\u001b[39m, os.getcwd())\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mmain_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackpropagation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmnist\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UiB/8. semester/Deep Learning/Project 1 /Project 1/tests_backpropagation.py:252\u001b[39m, in \u001b[36mmain_test\u001b[39m\u001b[34m(backprop_fn, model, eps, verbose, data)\u001b[39m\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (model.n_l[\u001b[32m0\u001b[39m] != \u001b[32m24\u001b[39m*\u001b[32m24\u001b[39m):\n\u001b[32m    251\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mPlease choose 576 (=24x24) as input dimension if using MNIST dataset\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m data = \u001b[43mload_MNIST\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m N=\u001b[32m5\u001b[39m\n\u001b[32m    254\u001b[39m inputs = [torch.unsqueeze(img, \u001b[32m0\u001b[39m).to(dtype=torch.double) \u001b[38;5;28;01mfor\u001b[39;00m img, _ \u001b[38;5;129;01min\u001b[39;00m data][:N]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UiB/8. semester/Deep Learning/Project 1 /Project 1/tests_backpropagation.py:128\u001b[39m, in \u001b[36mload_MNIST\u001b[39m\u001b[34m(data_path, preprocessor)\u001b[39m\n\u001b[32m    121\u001b[39m     preprocessor = transforms.Compose([\n\u001b[32m    122\u001b[39m         transforms.CenterCrop(\u001b[32m24\u001b[39m),\n\u001b[32m    123\u001b[39m         transforms.ToTensor(),\n\u001b[32m    124\u001b[39m         transforms.Normalize(\u001b[32m0.1306\u001b[39m, \u001b[32m0.3080\u001b[39m),\n\u001b[32m    125\u001b[39m     ])\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# load just test dataset because it's smaller\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m data_test = \u001b[43mdatasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMNIST\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreprocessor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data_test\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/torchvision/datasets/mnist.py:100\u001b[39m, in \u001b[36mMNIST.__init__\u001b[39m\u001b[34m(self, root, train, transform, target_transform, download)\u001b[39m\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_exists():\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDataset not found. You can use download=True to download it\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/torchvision/datasets/mnist.py:197\u001b[39m, in \u001b[36mMNIST.download\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mirror, err \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m.mirrors, errors):\n\u001b[32m    196\u001b[39m     s += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTried \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmirror\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, got:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(err)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(s)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error downloading train-images-idx3-ubyte.gz:\nTried https://ossci-datasets.s3.amazonaws.com/mnist/, got:\n<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1081)>\nTried http://yann.lecun.com/exdb/mnist/, got:\nHTTP Error 404: Not Found\n"
     ]
    }
   ],
   "source": [
    "model = MyNet([24*24, 16, 10])\n",
    "\n",
    "\n",
    "import os \n",
    "print('CWD:', os.getcwd())\n",
    "\n",
    "\n",
    "main_test(backpropagation, model, verbose=False, data='mnist')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
